{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minari\n",
    "import minari.dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import gym\n",
    "import glfw\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ret_to_go(episode, max_score):\n",
    "    ret_to_go = copy(episode.rewards)\n",
    "    for i in range(1, len(ret_to_go)):\n",
    "        ret_to_go[:-i] += episode.rewards[i:]\n",
    "    ret_to_go /= max_score # normalize\n",
    "    return ret_to_go\n",
    "\n",
    "class HistCNNDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset: minari.MinariDataset, max_score, tdim):\n",
    "        self.tdim = tdim\n",
    "        self.max_score = max_score\n",
    "        self.m_dataset = dataset\n",
    "        print(\"Observation Shape: \", dataset.spec.observation_space.shape)\n",
    "        print(\"Actions Shape: \", dataset.spec.action_space.shape)\n",
    "        print(\"RTG Shape: \", 1)\n",
    "        print(\"Net feed shape: \", 1 + dataset.spec.observation_space.shape[0] + dataset.spec.action_space.shape[0])\n",
    "\n",
    "        observations = []\n",
    "        actions = []\n",
    "\n",
    "        for episode in dataset.iterate_episodes():\n",
    "            observations.append(episode.observations)\n",
    "            actions.append(episode.actions)\n",
    "\n",
    "        observations = np.concatenate(observations)\n",
    "        actions = np.concatenate(actions)\n",
    "        self.max_obs = np.max(observations, axis=0, keepdims=True)\n",
    "        self.max_act = np.max(actions, axis=0, keepdims=True)\n",
    "        self.min_obs = np.min(observations, axis=0, keepdims=True)\n",
    "        self.min_act = np.min(actions, axis=0, keepdims=True)\n",
    "        print(\"Max obs: \", self.max_obs)\n",
    "        print(\"Min obs: \", self.min_obs)\n",
    "        print(\"Max act: \", self.max_act)\n",
    "        print(\"Min act: \", self.min_act)\n",
    "\n",
    "        obs_dim = dataset.spec.observation_space.shape[0]\n",
    "        self.action_dim = dataset.spec.action_space.shape[0]\n",
    "        ret_dim = 1\n",
    "        self.net_feed_dim = obs_dim + self.action_dim + ret_dim\n",
    "\n",
    "        self.lengths = []\n",
    "        self.ixds = []\n",
    "        self.pp_dataset = []\n",
    "        self.labels = []\n",
    "        for episode in dataset.iterate_episodes():\n",
    "            len_eps = episode.total_timesteps+1\n",
    "            net_feed = np.zeros(shape=(len_eps+tdim-1,self.net_feed_dim))\n",
    "            # Preprocess the data\n",
    "            pp_obs = (episode.observations - self.min_obs) / (self.max_obs - self.min_obs)\n",
    "            pp_act = ((episode.actions - self.min_act) / (self.max_act - self.min_act))\n",
    "            rtg = get_ret_to_go(episode, max_score)\n",
    "            # Fill in the data\n",
    "            net_feed[tdim-1:, :obs_dim] = pp_obs\n",
    "            net_feed[tdim:, obs_dim:obs_dim+self.action_dim] = pp_act\n",
    "            net_feed[tdim-1:-1, -1] = rtg\n",
    "            # Fill in the labels\n",
    "            self.labels.append(pp_act)\n",
    "            # Drop the last timestep because we don't have an action label for it\n",
    "            net_feed = net_feed[:-1]\n",
    "            len_eps -= 1\n",
    "            self.lengths.append(len_eps)\n",
    "            if not self.ixds:\n",
    "                self.ixds.append(list(range(len_eps)))\n",
    "            else:\n",
    "                self.ixds.append([self.ixds[-1][-1]+1 + i for i in range(len_eps)])\n",
    "            self.pp_dataset.append(net_feed)\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(self.lengths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        episode_idx = 0\n",
    "        while True:\n",
    "            if idx in self.ixds[episode_idx]:\n",
    "                break\n",
    "            episode_idx += 1\n",
    "        episode_internal_idx = self.ixds[episode_idx].index(idx)\n",
    "        return self.pp_dataset[episode_idx][episode_internal_idx:episode_internal_idx+self.tdim], self.labels[episode_idx][episode_internal_idx]\n",
    "\n",
    "    def make_obs(self, obs, act, ret_to_go):\n",
    "        if type(obs) == np.ndarray:\n",
    "            obs = torch.tensor(obs)\n",
    "        if type(act) == np.ndarray:\n",
    "            act = torch.tensor(act)\n",
    "        obs = (obs - self.min_obs) / (self.max_obs - self.min_obs)\n",
    "        rtg = ret_to_go / self.max_score\n",
    "        return torch.concatenate([obs, act, torch.tensor([[rtg]], dtype=torch.float32)], dim=1)\n",
    "\n",
    "    def unwrap_action(self, NN_action):\n",
    "        return NN_action * (self.max_act - self.min_act) + self.min_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryCNN(torch.nn.Module):\n",
    "    def __init__(self, tdim, obs_dim, act_dim, num_filters):\n",
    "        super(HistoryCNN, self).__init__()\n",
    "        assert tdim >= 2\n",
    "        assert obs_dim > 0\n",
    "        assert num_filters > 0\n",
    "        \n",
    "        self.act_dim = act_dim\n",
    "        self.obs_dim = obs_dim\n",
    "        self.num_filters = num_filters\n",
    "        self.tdim = tdim\n",
    "\n",
    "        ksize = 2\n",
    "        kernel_sizes = [ksize]\n",
    "        dim = tdim - ksize + 1\n",
    "        ksize += 1\n",
    "        while dim >= ksize:\n",
    "            kernel_sizes.append(kernel_sizes[-1] + 1)\n",
    "            dim -= kernel_sizes[-1] - 1\n",
    "            ksize += 1\n",
    "\n",
    "        if dim > 1:\n",
    "            kernel_sizes.append(dim)\n",
    "\n",
    "        print(\"kernel sizes: \", kernel_sizes)\n",
    "        modules = []\n",
    "        for i, ksize in enumerate(kernel_sizes):\n",
    "            if i == 0:\n",
    "                modules.append((f\"conv {i+1}\", torch.nn.Conv1d(in_channels=obs_dim, out_channels=num_filters, kernel_size=ksize)))\n",
    "            else:\n",
    "                modules.append((f\"conv {i+1}\", torch.nn.Conv1d(in_channels=num_filters, out_channels=num_filters, kernel_size=ksize)))\n",
    "            if i != len(kernel_sizes) - 1:\n",
    "                modules.append((f\"bnorm {i+1}\", torch.nn.BatchNorm1d(num_filters)))\n",
    "            modules.append((f\"relu {i+1}\", torch.nn.ReLU()))\n",
    "        \n",
    "        modules.append((\"flatten\", torch.nn.Flatten()))\n",
    "        modules.append((\"fc\", torch.nn.Linear(num_filters, act_dim)))\n",
    "        modules.append((\"tanh\", torch.nn.Tanh()))\n",
    "        self.conv = torch.nn.Sequential(OrderedDict(modules))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model: HistoryCNN, train_dataset: HistCNNDataset, target_rtg: float, n_test_runs: int=10, device='cpu', render=False):\n",
    "    model.to(device)\n",
    "    m_dataset = train_dataset.m_dataset\n",
    "    if render:\n",
    "        assert n_test_runs == 1\n",
    "        env = gym.make(m_dataset._data.env_spec.id, render_mode='human')\n",
    "        glfw.init()\n",
    "    else:\n",
    "        env = gym.make(m_dataset._data.env_spec.id)\n",
    "    n_scores = []\n",
    "    for seed in range(n_test_runs):\n",
    "        raw_obs, _ = env.reset(seed=seed+12341234)\n",
    "        last_action = torch.zeros(1, model.act_dim)\n",
    "        running_obs = torch.zeros(1, model.tdim, model.obs_dim)\n",
    "        pp_obs = train_dataset.make_obs(raw_obs, last_action, target_rtg)\n",
    "        running_obs[0, -1] = pp_obs\n",
    "        model.eval()\n",
    "        done = False\n",
    "\n",
    "        ret = 0\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()\n",
    "            with torch.no_grad():\n",
    "                action = model(running_obs.transpose(1,2))\n",
    "                last_action = copy(action)\n",
    "                action = train_dataset.unwrap_action(action)\n",
    "            \n",
    "            raw_obs, reward, ter, trunc, _ = env.step(action.squeeze().numpy())\n",
    "            done = ter or trunc\n",
    "            target_rtg -= reward\n",
    "            ret += reward\n",
    "            pp_obs = train_dataset.make_obs(raw_obs, last_action, target_rtg)\n",
    "            running_obs[0, :-1] = running_obs[0, 1:].clone()\n",
    "            running_obs[0, -1] = pp_obs\n",
    "        n_score = minari.get_normalized_score(m_dataset, ret)\n",
    "        n_scores.append(n_score)\n",
    "    env.close()\n",
    "    glfw.terminate()\n",
    "    return np.average(n_scores), np.std(n_scores), n_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, device, n_epochs=100):\n",
    "    # Initialize TensorBoard SummaryWriter\n",
    "    log_dir = \"runs/experiment\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "    net = model\n",
    "\n",
    "    # Set up loss criterion, optimizer, and other configurations\n",
    "    criterion = torch.nn.MSELoss().to(device).float()  # Ensure the criterion is in float32\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "    epochs = n_epochs\n",
    "    best_loss = float('inf')  # Initialize with a high value\n",
    "\n",
    "    # Convert model to float32 and move to device\n",
    "    net = net.to(device).float()\n",
    "\n",
    "    # Create directory to save models\n",
    "    model_dir = \"models\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # Training loop with tqdm progress bar\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        net = net.to(device).float()\n",
    "        # Initialize the progress bar for each epoch\n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\") as pbar:\n",
    "            for i, data in enumerate(train_loader):\n",
    "                inputs, labels = data\n",
    "                \n",
    "                # Convert inputs and labels to float32 and move to device\n",
    "                inputs = inputs.float().transpose(1, 2).to(device)\n",
    "                labels = labels.float().to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Accumulate loss for logging\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.set_postfix({\"Loss\": loss.item()})\n",
    "                pbar.update(1)\n",
    "\n",
    "                # Log to TensorBoard every 10 batches\n",
    "                if i % 10 == 0:\n",
    "                    writer.add_scalar('Training Loss', loss.item(), epoch * len(train_loader) + i)\n",
    "\n",
    "        # Calculate and log average loss for the epoch\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss}\")\n",
    "        writer.add_scalar('Average Loss per Epoch', avg_loss, epoch)\n",
    "\n",
    "        # Save model at the end of each epoch\n",
    "        torch.save(net.state_dict(), os.path.join(model_dir, f\"model_epoch_{epoch+1}.pth\"))\n",
    "\n",
    "        # Check if this is the best model so far and save it if it is\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(net.state_dict(), os.path.join(model_dir, \"best_model.pth\"))\n",
    "            print(f\"New best model saved with loss {best_loss}\")\n",
    "            n_score = eval_model(model, train_loader.dataset, target_rtg=3600, n_test_runs=50)\n",
    "            print(f\"Current normalized score: {100 * n_score}/100\")\n",
    "\n",
    "    # Close the TensorBoard writer when training is done\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  mps\n",
      "Observation Shape:  (11,)\n",
      "Actions Shape:  (3,)\n",
      "RTG Shape:  1\n",
      "Net feed shape:  15\n",
      "Max obs:  [[ 1.729025    0.1552431   0.01719327  0.07019253  0.95734406  5.017929\n",
      "   3.1744173   3.8315835   5.686337    9.226352   10.        ]]\n",
      "Min obs:  [[  0.65998334  -0.19498888  -1.4958394   -1.5184437   -0.96395123\n",
      "   -0.503773    -5.3455234   -3.20404     -6.4856668  -10.\n",
      "  -10.        ]]\n",
      "Max act:  [[0.9998843 0.9999483 0.9999945]]\n",
      "Min act:  [[-0.9999679  -0.9999835  -0.99996823]]\n",
      "kernel sizes:  [2, 3, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "tdim = 10\n",
    "batch_size = 64\n",
    "num_filters = 2\n",
    "m_dataset = dataset = minari.load_dataset(\"hopper-medium-v2\")\n",
    "max_score = 3600\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device: \", device)\n",
    "train_dataset = HistCNNDataset(m_dataset, max_score, tdim)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "testdata = torch.rand(1, 15, 20)\n",
    "net = HistoryCNN(tdim=tdim, obs_dim=train_dataset.net_feed_dim, act_dim=train_dataset.action_dim, num_filters=num_filters).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lg/78k356557nndnksbfbmlymq40000gn/T/ipykernel_38893/4228741106.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(\"models/model_epoch_4.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"models/model_epoch_4.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   6%|â–Œ         | 917/15625 [03:35<57:29,  4.26batch/s, Loss=0.14]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[45], line 26\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, device, n_epochs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Initialize the progress bar for each epoch\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     27\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Convert inputs and labels to float32 and move to device\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/adrl/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/adrl/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/adrl/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/adrl/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[3], line 74\u001b[0m, in \u001b[0;36mHistCNNDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     72\u001b[0m episode_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mixds[episode_idx]:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     episode_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train_model(net, train_loader, device, n_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final normalized score: 21.2+-13.4\n"
     ]
    }
   ],
   "source": [
    "score_m, score_std, n_scores = eval_model(net, train_dataset, target_rtg=3600, n_test_runs=50, render=False)\n",
    "print(f\"Final normalized score: {100 * score_m:.1f}+-{100 * score_std:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n",
      "Final normalized score: 29.8\n"
     ]
    }
   ],
   "source": [
    "score = eval_model(net, train_dataset, target_rtg=3600, n_test_runs=1, render=True)\n",
    "print(f\"Final normalized score: {100 * score_m:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
